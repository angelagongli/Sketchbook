<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Magenta</title>
    <script src="https://cdn.jsdelivr.net/npm/@magenta/music@^1.0.0"></script>
</head>
<body>
    All from <a href="https://hello-magenta.glitch.me/">https://hello-magenta.glitch.me/</a><br>
    <canvas id="sequenceCanvas" width="500" height="500"></canvas><br>
    <button onclick="startOrStop(event, sequencePlayer, TWINKLE_TWINKLE)">Play</button><br>
    <canvas id="chordCanvas" width="500" height="500"></canvas><br>
    <button onclick="startOrStop(event, chordPlayer, MAJOR_CHORD)">Play</button><br>
    <h1>MusicRNN</h1>
    <p>"A MusicRNN is an LSTM-based language model for musical notes -- it is best at continuing a NoteSequence that you give it."</p>
    <button onclick="playMusicRNN()">Play MusicRNN</button><br>
    <h1>MusicVAE</h1>
    <p>"A MusicVAE is a variational autoencoder made up of an Encoder and Decoder -- you can think of the encoder as trying to summarize all the data you give it, and the decoder as trying to recreate the original data, based on this summarized version."</p>
    <button onclick="playVAE()">Play MusicVAE</button>
    <script>
        TWINKLE_TWINKLE = {
            notes: [
                {pitch: 60, startTime: 0.0, endTime: 0.5},
                {pitch: 60, startTime: 0.5, endTime: 1.0},
                {pitch: 67, startTime: 1.0, endTime: 1.5},
                {pitch: 67, startTime: 1.5, endTime: 2.0},
                {pitch: 69, startTime: 2.0, endTime: 2.5},
                {pitch: 69, startTime: 2.5, endTime: 3.0},
                {pitch: 67, startTime: 3.0, endTime: 4.0},
                {pitch: 65, startTime: 4.0, endTime: 4.5},
                {pitch: 65, startTime: 4.5, endTime: 5.0},
                {pitch: 64, startTime: 5.0, endTime: 5.5},
                {pitch: 64, startTime: 5.5, endTime: 6.0},
                {pitch: 62, startTime: 6.0, endTime: 6.5},
                {pitch: 62, startTime: 6.5, endTime: 7.0},
                {pitch: 60, startTime: 7.0, endTime: 8.0},
            ],
            totalTime: 8
        };
        MAJOR_CHORD = {
            notes: [
                {pitch: 60, startTime: 0.0, endTime: 1.0},
                {pitch: 64, startTime: 0.33, endTime: 1.0},
                {pitch: 67, startTime: 0.67, endTime: 1.0}
            ],
            totalTime: 1
        };
        sequenceConfig = {
            noteHeight: 20,
            pixelsPerTimeStep: 20,
            noteSpacing: 2,
            noteRGB: '100, 0, 100',
            activeNoteRGB: '200, 0, 200'
        };
        chordConfig = {
            noteHeight: 11,
            pixelsPerTimeStep: 50,
            noteSpacing: 1,
            noteRGB: '0, 100, 100',
            activeNoteRGB: '0, 200, 200'
        };
        sequenceVisualization = new mm.PianoRollCanvasVisualizer(TWINKLE_TWINKLE, document.getElementById('sequenceCanvas'), sequenceConfig);
        chordVisualization = new mm.PianoRollCanvasVisualizer(MAJOR_CHORD, document.getElementById('chordCanvas'), chordConfig);
        sequencePlayer = new mm.Player(false, {
            run: note => sequenceVisualization.redraw(note),
            stop: () => {
                console.log('Done');
            }
        });
        chordPlayer = new mm.Player(false, {
            run: note => chordVisualization.redraw(note),
            stop: () => {
                console.log('Done');
            }
        });
        function startOrStop(event, p, seq) {
            if (p.isPlaying()) {
                p.stop();
                event.target.textContent = 'Play';
            } else {
                p.start(seq).then(() => {
                    const btns = document.querySelectorAll('.controls > button');
                    for (let btn of btns) {
                        btn.textContent = 'Play';
                    }
                });
                event.target.textContent = 'Stop';
            }
        }

        // MusicRNN
        // Initialize the model.
        music_rnn = new mm.MusicRNN('https://storage.googleapis.com/magentadata/js/checkpoints/music_rnn/basic_rnn');
        music_rnn.initialize();

        // Create a player to play the sequence we'll get from the model.
        rnnPlayer = new mm.Player();

        rnn_steps = 20;
        rnn_temperature = 1.5;

        function playMusicRNN() {
            if (rnnPlayer.isPlaying()) {
                rnnPlayer.stop();
                return;
            }
                
            // The model expects a quantized sequence, and ours was unquantized:
            const qns = mm.sequences.quantizeNoteSequence(TWINKLE_TWINKLE, 4);
            music_rnn
            .continueSequence(qns, rnn_steps, rnn_temperature)
            .then((sample) => rnnPlayer.start(sample));
        }

        // MusicVAE
        // Initialize the model.
        music_vae = new mm.MusicVAE('https://storage.googleapis.com/magentadata/js/checkpoints/music_vae/mel_4bar_small_q2');
        music_vae.initialize();

        // Create a player to play the sampled sequence.
        vaePlayer = new mm.Player();

        vae_temperature = 1.5;

        function playVAE() {
            if (vaePlayer.isPlaying()) {
                vaePlayer.stop();
                return;
            }
            music_vae
            .sample(1, vae_temperature)
            .then((sample) => vaePlayer.start(sample[0]));
        }
    </script>
</body>
</html>
